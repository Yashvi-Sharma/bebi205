{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import datasets\n",
    "ds = datasets.load_dataset(\"damlab/uniprot\")\n",
    "ds[\"train\"][\"sequence\"][0] # Inspect the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4da146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AdamW, pipeline, RobertaTokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer, PreTrainedTokenizerFast\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make iterator for tokenizing\n",
    "def generate_iter():\n",
    "    for i in range(0,ds['train'].num_rows,10000):\n",
    "        seq = ds['train']['sequence'][i:i + 10000]\n",
    "        yield seq\n",
    "vocab = ['A','R','N','D','C','E','Q','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "corpus = generate_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(vocab=vocab)\n",
    "tokenizer.train_from_iterator(corpus,show_progress=True,vocab_size=100,\n",
    "                              special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n",
    "tokenizer.save_model(\".\", \"ast3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73780e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tokenizer for Roberta\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\"ast3-vocab.json\",\"ast3-merges.txt\")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing((\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "print(tokenizer.get_vocab_size())\n",
    "tokenizer.enable_truncation(max_length=256)\n",
    "tokenizer.enable_padding()\n",
    "tokenizer.save('token/ast3')\n",
    "tokenizer.save_model('token','ast3')\n",
    "tokenizer = RobertaTokenizerFast(vocab_file=\"token/ast3-vocab.json\",merges_file=\"token/ast3-merges.txt\")\n",
    "tokenizer.save_pretrained('token/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('token/')\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(ds['train']['sequence'][1000],max_length=256,\n",
    "                               truncation=True,padding='max_length') # test tokenizer\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03151ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <mask> is 4, <pad> is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loader class for pytorch taken directly from a website\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, tokenizer, i_start, i_end):\n",
    "        self.encodings = []\n",
    "#         self.masks = []\n",
    "        for seq in ds['train']['sequence'][i_start:i_end]:\n",
    "            seq_encoded = tokenizer.encode_plus(seq, max_length = 256, truncation=True, padding='max_length')\n",
    "            self.encodings += [seq_encoded.input_ids]\n",
    "#             self.masks += [seq_encoded.attention_mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return torch.tensor(self.encodings[i])\n",
    "\n",
    "trdataset = Dataset(ds,tokenizer,0,10000)\n",
    "evdataset = Dataset(ds, tokenizer, 10000, 11000)\n",
    "loader = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evdataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e492498",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ef21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BertConfig(vocab_size=100,max_position_embeddings=256)\n",
    "# model = BertForMaskedLM(config)\n",
    "config = RobertaConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=258, \n",
    "                       type_vocab_size=1, hidden_size=768, num_attention_heads=12, \n",
    "                       num_hidden_layers=6,) ## Sneaky max pos embedding = max len+2\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "targs = TrainingArguments(output_dir='ast',overwrite_output_dir=True,evaluation_strategy='epoch',\n",
    "                         num_train_epochs=2,learning_rate=1e-2,per_device_train_batch_size=64,\n",
    "                         per_device_eval_batch_size=32, save_total_limit=1)\n",
    "trainer = Trainer(model=model,args=targs,data_collator=loader,train_dataset=trdataset,eval_dataset=evdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./ast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = pipeline('fill-mask', model='./ast', tokenizer='token/', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill(f'MAFSAE<mask>VLKEYDRRRRMEALLLSLYYP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model performance not looking great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9216784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('./ast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fca392",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "masks = []\n",
    "for seq in vocab:\n",
    "    seq_encoded = tokenizer.encode_plus(seq, \n",
    "                            max_length = 256, truncation=True, padding='max_length')\n",
    "    ids.append(seq_encoded.input_ids)\n",
    "    masks.append(seq_encoded.attention_mask)\n",
    "ids = torch.tensor(ids)\n",
    "masks = torch.tensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting vocab embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fec9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(ids,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = out[0]\n",
    "lhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = lhs[:,0,:].detach()\n",
    "cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49076eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "vis = pca.fit_transform(cls)\n",
    "vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f44c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting colors and markers same as example figure\n",
    "pcharge = ['K','R','H']\n",
    "ncharge = ['D','E']\n",
    "hydrophobic = ['A','I','L','M','V']\n",
    "aromatic = ['F','W','Y']\n",
    "polar = ['S','T','N','Q','H']\n",
    "unique = ['C','G','P']\n",
    "small = ['A','G','P','S','T','V']\n",
    "med = ['C','I','M','L','N','Q','K','D','E']\n",
    "large = ['H','R','F','W','Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594329f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for j in range(20):\n",
    "    aa = vocab[j]\n",
    "    if aa in pcharge:\n",
    "        color = 'red'\n",
    "        marker = 's'\n",
    "        label = 'Positively charged'\n",
    "    elif aa in ncharge:\n",
    "        color='red'\n",
    "        marker='x'\n",
    "        label = 'Negatively charged'\n",
    "    elif aa in hydrophobic:\n",
    "        color='green'\n",
    "        marker = 'o'\n",
    "        label = 'Hydrophobic'\n",
    "    elif aa in aromatic:\n",
    "        color='green'\n",
    "        marker='+'\n",
    "        label = 'Aromatic'\n",
    "    elif aa in polar:\n",
    "        color='blue'\n",
    "        marker='o'\n",
    "        label = 'Polar'\n",
    "    elif aa in unique:\n",
    "        color='orange'\n",
    "        marker='o'\n",
    "        label = 'Unique'\n",
    "    if aa in polar and aa in pcharge:\n",
    "        color='purple'\n",
    "        label = aa\n",
    "    if aa in small:\n",
    "        s = 30\n",
    "    elif aa in med:\n",
    "        s = 60\n",
    "    elif aa in large:\n",
    "        s = 90\n",
    "    plt.scatter(vis[j,0],vis[j,1],label=label,color=color,marker=marker,s=s)\n",
    "plt.xlabel('PCA 0')\n",
    "plt.ylabel('PCA 1')\n",
    "h,l = plt.gca().get_legend_handles_labels()\n",
    "ln = np.unique(l)\n",
    "hn = []\n",
    "lnn = []\n",
    "for li in ln:\n",
    "    print(li)\n",
    "    hn.append(h[np.where(np.array(l)==li)[0][0]])\n",
    "    lnn.append(li)\n",
    "plt.legend(handles=hn,labels=lnn,loc=(1.01,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ad9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8577273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef779b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acdac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
